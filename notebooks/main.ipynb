{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcd34ffb",
   "metadata": {},
   "source": [
    "# PM2.5 Prediction Analysis\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Air pollution is a critical environmental issue affecting millions of people worldwide, with particularly severe impacts in rapidly developing urban areas. Beijing, one of the world's largest cities and a major economic hub, has been grappling with significant air quality challenges for years. Among the various pollutants, PM2.5 (fine particulate matter with a diameter of 2.5 micrometers or less) is of particular concern due to its severe health impacts.\n",
    "\n",
    "### The Impact of PM2.5\n",
    "PM2.5 particles are small enough to penetrate deep into the lungs and even enter the bloodstream, leading to a range of health problems including:\n",
    "\n",
    "- Respiratory issues (asthma, bronchitis)\n",
    "- Cardiovascular diseases\n",
    "- Reduced lung function\n",
    "- Increased risk of stroke\n",
    "- Premature death in people with heart or lung disease\n",
    "\n",
    "Beyond health impacts, high levels of PM2.5 also affect visibility, climate, and can have significant economic consequences due to increased healthcare costs and reduced productivity.\n",
    "\n",
    "## 1. Import Libraries\n",
    "\n",
    "In this section, we import all necessary libraries for data manipulation, visualization, machine learning, and time series analysis. These libraries include pandas, numpy, matplotlib, seaborn, scikit-learn, and others specific to time series analysis and advanced machine learning models like XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f23157",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#import linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# import decision tree\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "import xgboost as xgb\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statsmodels.tsa.stattools import adfuller, acf\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from scipy import stats\n",
    "\n",
    "import joblib\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed19040f",
   "metadata": {},
   "source": [
    "## 2. Load and Preprocess Data\n",
    "\n",
    "Our analysis uses a comprehensive dataset that combines PM2.5 measurements from the U.S. Embassy in Beijing with meteorological data from Beijing Capital International Airport. This hourly dataset provides a rich source of information for understanding the interplay between various environmental factors and PM2.5 concentrations.\n",
    "\n",
    "### Dataset Features:\n",
    "\n",
    "1. **Temporal Information**: Year, Month, Day, Hour\n",
    "2. **Target Variable**: PM2.5 concentration\n",
    "3. **Meteorological Factors**: Dew Point, Temperature, Pressure, Wind direction and speed\n",
    "4. **Precipitation Indicators**: Cumulated hours of snow and rain\n",
    "\n",
    "In this section, we:\n",
    "1. Load the CSV file containing Beijing PM2.5 data.\n",
    "2. Define and apply the `preprocess_data` function to clean the data, handle missing values, and create datetime features.\n",
    "3. Create dataframe `X` for feature-based models.\n",
    "4. Check for Seasonality and Stationarity\n",
    "\n",
    "   Here, we perform checks for seasonality and stationarity in the PM2.5 time series:\n",
    "   1. The `check_seasonality` function performs multiple checks: \n",
    "      - Seasonal decomposition\n",
    "      - Autocorrelation\n",
    "      - Statistical test for seasonality\n",
    "   2. The `check_stationarity` function uses the Augmented Dickey-Fuller test to check for stationarity.\n",
    "   3. We apply both functions to our PM2.5 data to understand its temporal characteristics.\n",
    "   \n",
    "5. Data Splitting\n",
    "\n",
    "   In this step, we split our data:\n",
    "   1. We create a test set (`X_test_final`) containing data from December 28, 2014 to December 31, 2014. These two are the last days of the dataset and roughly represent the 100 data points that I need to predict with my mode (24*4)\n",
    "   \n",
    "   2. The remaining data is kept in `X` for our main analysis and model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08858ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-01T21:29:33.708734Z",
     "iopub.status.busy": "2022-08-01T21:29:33.708337Z",
     "iopub.status.idle": "2022-08-01T21:29:33.962861Z",
     "shell.execute_reply": "2022-08-01T21:29:33.961793Z"
    },
    "papermill": {
     "duration": 0.268166,
     "end_time": "2022-08-01T21:29:33.965715",
     "exception": false,
     "start_time": "2022-08-01T21:29:33.697549",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for docker\n",
    "df = pd.read_csv('beijing_pm_25.csv')\n",
    "display(df.head())\n",
    "\n",
    "# for local\n",
    "#df = pd.read_csv('/Users/andresjr/Documents/capgemini/Task_2_new/data/beijing_pm_25.csv')\n",
    "#display(df.head())\n",
    "\n",
    "\n",
    "def preprocess_data(df):\n",
    "\n",
    "    pd.options.mode.chained_assignment = None\n",
    "    # Drop rows with missing target values\n",
    "    print(\"Shape before dropping missing values:\", df.shape)\n",
    "    df = df.dropna(subset=['pm2.5'])\n",
    "\n",
    "    #print(\"Shape before dropping missing values:\", df.shape)\n",
    "    #drop rows with missing values\n",
    "    df = df.dropna()\n",
    "    print(\"Shape after dropping missing values:\", df.shape)\n",
    "\n",
    "        # Combine columns into a datetime object\n",
    "    df['datetime'] = pd.to_datetime(df[['year', 'month', 'day', 'hour']])\n",
    "\n",
    "    # Convert to the desired format\n",
    "    df['date'] = df['datetime'].dt.strftime('%Y-%m-%d %H:00:00').astype('datetime64[ns]')\n",
    "\n",
    "    # Drop the original columns\n",
    "    df = df.drop(['year', 'month', 'day', 'hour', 'datetime'], axis=1)\n",
    "\n",
    "    # Select features\n",
    "    features = ['DEWP', 'TEMP', 'PRES', 'Iws']\n",
    "\n",
    "    y = df['pm2.5']\n",
    "    X = df[features]\n",
    "\n",
    "    # Add the date column back to the dataframe as the first column do not use insert\n",
    "    X['date'] = df['date']\n",
    "    X['pm2.5'] = y\n",
    "    \n",
    "    X = X.set_index('date')\n",
    "\n",
    "    return X\n",
    "\n",
    "def check_seasonality(y):\n",
    "\n",
    "    \"\"\"\n",
    "    Perform multiple checks for seasonality in the time series.\n",
    "    \"\"\"\n",
    "    \n",
    "    color_pal = sns.color_palette()\n",
    "    plt.style.use('fivethirtyeight')\n",
    "    \n",
    "    # 1. Visual check with seasonal decomposition\n",
    "    result = seasonal_decompose(y, model='additive', period=365)\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.subplot(411)\n",
    "    plt.plot(result.observed)\n",
    "    plt.title('Observed')\n",
    "    plt.subplot(412)\n",
    "    plt.plot(result.trend)\n",
    "    plt.title('Trend')\n",
    "    plt.subplot(413)\n",
    "    plt.plot(result.seasonal)\n",
    "    plt.title('Seasonal')\n",
    "    plt.subplot(414)\n",
    "    plt.plot(result.resid)\n",
    "    plt.title('Residual')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Autocorrelation check\n",
    "    lag_acf = acf(y, nlags=365)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(lag_acf)\n",
    "    plt.title('Autocorrelation Function')\n",
    "    plt.axhline(y=0, linestyle='--', color='gray')\n",
    "    plt.axhline(y=-1.96/np.sqrt(len(y)), linestyle='--', color='gray')\n",
    "    plt.axhline(y=1.96/np.sqrt(len(y)), linestyle='--', color='gray')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 4. Statistical test for seasonality\n",
    "    y_diff = y.diff().dropna()\n",
    "    y_diff_square = y_diff ** 2\n",
    "    result = stats.pearsonr(y_diff_square[:-12], y_diff_square[12:])\n",
    "    print(f\"Pearson correlation for seasonality: {result[0]:.4f}\")\n",
    "    print(f\"p-value: {result[1]:.4f}\")\n",
    "    if result[1] < 0.05:\n",
    "        print(\"The time series likely has seasonality (p < 0.05)\")\n",
    "    else:\n",
    "        print(\"No strong evidence of seasonality (p >= 0.05)\")\n",
    "\n",
    "def check_stationarity(y):\n",
    "\n",
    "    color_pal = sns.color_palette()\n",
    "    plt.style.use('fivethirtyeight')\n",
    "    \n",
    "    result = adfuller(y)\n",
    "    print('ADF Statistic:', result[0])\n",
    "    print('p-value:', result[1])\n",
    "    print('Critical Values:', result[4])\n",
    "    if result[1] <= 0.05:\n",
    "        print(\"The series is stationary\")\n",
    "    else:\n",
    "        print(\"The series is not stationary\")\n",
    "\n",
    "\n",
    "X = preprocess_data(df)\n",
    "display(X.head())\n",
    "\n",
    "# Check for seasonality\n",
    "check_seasonality(X['pm2.5'])\n",
    "# Check for stationarity\n",
    "check_stationarity(X['pm2.5'])\n",
    "\n",
    "print(\"Latest datetime record (will be used for test set):\", X.index.max())\n",
    "\n",
    "\n",
    "X_ = X[X.index < '2014-12-28']\n",
    "\n",
    "X_test_final = X[X.index >= '2014-12-28']\n",
    "\n",
    "X = X_.copy()\n",
    "display(X_test_final.head())\n",
    "print(X.index.max())\n",
    "print(\"Latest datetime record training set:\", X.index.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c1f3cd",
   "metadata": {},
   "source": [
    "## 3. Time Series Cross-Validation\n",
    "\n",
    "Here, we implement and example of the time series cross-validation strategy we will follow: \n",
    "\n",
    "1. We use `TimeSeriesSplit` from scikit-learn to create a time series cross-validation strategy.\n",
    "2. The data is split into 8 folds, with each test set being 60 days (with 24 hours each) in length.\n",
    "3. We visualize each fold to understand how the data is being split over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c63221",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-01T21:29:35.520674Z",
     "iopub.status.busy": "2022-08-01T21:29:35.519981Z",
     "iopub.status.idle": "2022-08-01T21:29:36.743780Z",
     "shell.execute_reply": "2022-08-01T21:29:36.742417Z"
    },
    "papermill": {
     "duration": 1.238946,
     "end_time": "2022-08-01T21:29:36.746421",
     "exception": false,
     "start_time": "2022-08-01T21:29:35.507475",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "n = 8\n",
    "tss = TimeSeriesSplit(n_splits=n, test_size=24*60*1, gap=0)\n",
    "X = X.sort_index()\n",
    "\n",
    "fig, axs = plt.subplots(n, 1, figsize=(25, 25), sharex=True)\n",
    "\n",
    "fold = 0\n",
    "for train_idx, val_idx in tss.split(X):\n",
    "    train = X.iloc[train_idx]\n",
    "    test = X.iloc[val_idx]\n",
    "    train['pm2.5'].plot(ax=axs[fold],\n",
    "                          label='Training Set',\n",
    "                          title=f'Data Train/Test Split Fold {fold}')\n",
    "    test['pm2.5'].plot(ax=axs[fold],\n",
    "                         label='Test Set')\n",
    "    axs[fold].axvline(test.index.min(), color='black', ls='--')\n",
    "    fold += 1\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8994f1a4",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering\n",
    "\n",
    "In this section, we perform feature engineering:\n",
    "1. The `create_features` function adds time-based features like hour, day of week, month, etc.\n",
    "2. The `add_lags` function creates lag features (previous year's PM2.5 values) to capture yearly patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6211d656",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df):\n",
    "    \"\"\"\n",
    "    Create time series features based on time series index.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['hour'] = df.index.hour\n",
    "    df['dayofweek'] = df.index.dayofweek\n",
    "    df['quarter'] = df.index.quarter\n",
    "    df['month'] = df.index.month\n",
    "    df['year'] = df.index.year\n",
    "    df['dayofyear'] = df.index.dayofyear\n",
    "    df['dayofmonth'] = df.index.day\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_lags(df):\n",
    "    target_map = df['pm2.5'].to_dict()\n",
    "    df['lag1'] = (df.index - pd.Timedelta('91 days')).map(target_map)\n",
    "    df['lag2'] = (df.index - pd.Timedelta('182 days')).map(target_map)\n",
    "    df['lag3'] = (df.index - pd.Timedelta('364 days')).map(target_map)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49abc148",
   "metadata": {},
   "source": [
    "## 5. Model Training and Evaluation\n",
    "\n",
    "Here, we define functions for model training, evaluation, and prediction:\n",
    "\n",
    "1. `evaluate_model`: Computes various performance metrics (MSE, RMSE, MAE, R2) for a given model.\n",
    "2. `train_model`: Implements the time series cross-validation strategy to train and evaluate multiple models.\n",
    "3. `metrics_to_df`: Converts the performance metrics into a pandas DataFrame for easy visualization.\n",
    "4. `future_predictions`: Uses a trained model to make predictions on the test set and visualize the results.\n",
    "5. `create_folder_structure`: Creates folder structure for later store of models in pkl format and the plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6928a4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(models, X, FEATURES, tss):\n",
    "    metrics = {model: {'RMSE':[],'MAE':[],'MSE':[],'R2':[]} for model in models.keys()}\n",
    "\n",
    "    preds = {'RMSE':[],'MAE':[],'MSE':[],'R2':[]}\n",
    "\n",
    "    for train_idx, val_idx in tss.split(X):\n",
    "        train = X.iloc[train_idx]\n",
    "        test = X.iloc[val_idx]\n",
    "\n",
    "        train = create_features(train)\n",
    "        test = create_features(test) \n",
    "        \n",
    "        model_store = {}\n",
    "        for name, model in models.items():\n",
    "\n",
    "            if name == \"XGBoost\":\n",
    "                train = add_lags(train)\n",
    "                test = add_lags(test)\n",
    "                FEATURES_copy = FEATURES.copy()\n",
    "                FEATURES += ['lag1','lag2','lag3']\n",
    "            \n",
    "            else:\n",
    "                # remove lags for other models\n",
    "                FEATURES = FEATURES_copy\n",
    "            \n",
    "            TARGET = 'pm2.5'\n",
    "\n",
    "            X_train = train[FEATURES]\n",
    "            y_train = train[TARGET]\n",
    "\n",
    "            X_test = test[FEATURES]\n",
    "            y_test = test[TARGET]\n",
    "\n",
    "            preds = {'RMSE':[],'MAE':[],'MSE':[],'R2':[]}\n",
    "\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            y_pred = model.predict(X_test)\n",
    "            \n",
    "            metrics_fold = evaluate_model(model,y_test,y_pred)\n",
    "\n",
    "            for i in preds.keys():\n",
    "                    metrics[name][i].append(metrics_fold[i])\n",
    "\n",
    "            model_store[name] = model\n",
    "\n",
    "    #Take the average of the metrics for each fold\n",
    "    for model_name in models.keys():\n",
    "        for i in preds.keys():\n",
    "            metrics[model_name][i] = np.mean(metrics[model_name][i])\n",
    "\n",
    "    return model_store, metrics\n",
    "\n",
    "def metrics_to_df(metrics):\n",
    "        #convert the metrics to a dataframe\n",
    "    metrics_df = pd.DataFrame(metrics)\n",
    "    metrics_df = metrics_df.transpose()\n",
    "    metrics_df = metrics_df.reset_index()\n",
    "    metrics_df = metrics_df.rename(columns={'index':'model'})\n",
    "    #sort the dataframe by RMSE\n",
    "    metrics_df = metrics_df.sort_values(by='RMSE')\n",
    "    display(metrics_df)\n",
    "    return metrics_df\n",
    "\n",
    "def future_predictions(best_model_name,best_model,df,X_test_final):\n",
    "\n",
    "    future_df = X_test_final\n",
    "    future_df['isFuture'] = True\n",
    "    df['isFuture'] = False\n",
    "    df_and_future = pd.concat([df, future_df])\n",
    "    df_and_future = create_features(df_and_future)\n",
    "    if best_model_name == \"XGBoost\":\n",
    "        df_and_future = add_lags(df_and_future)\n",
    "\n",
    "    #display(X_test_final)\n",
    "\n",
    "    FEATURES = ['DEWP', 'TEMP', 'PRES', 'Iws', 'hour', 'dayofweek', 'quarter', 'month', 'year', 'dayofyear', 'dayofmonth']\n",
    "\n",
    "    if best_model_name == \"XGBoost\":\n",
    "        FEATURES += ['lag1','lag2','lag3']\n",
    "\n",
    "    print(f\"Features used for prediction: {FEATURES}\")\n",
    "    future_w_features = df_and_future.query('isFuture').copy()\n",
    "    #print(future_w_features.columns)\n",
    "\n",
    "    # remove the target column\n",
    "    future_w_features = future_w_features.drop(columns=['pm2.5','isFuture','dayofmonth'])\n",
    "\n",
    "    #print(future_w_features.columns)\n",
    "\n",
    "    \n",
    "    future_w_features['pred'] = best_model.predict(future_w_features)\n",
    "\n",
    "    # compute the metrics for the future predictions MAE, RMSE, MSE, R2\n",
    "    metrics_future = evaluate_model(best_model,X_test_final['pm2.5'],future_w_features['pred'])\n",
    "    pred_metrics = pd.DataFrame(metrics_future,index=[0])\n",
    "    pred_metrics = pred_metrics.transpose()\n",
    "    pred_metrics = pred_metrics.reset_index()\n",
    "    pred_metrics = pred_metrics.rename(columns={'index':'metric',0:'value'})\n",
    "\n",
    "    model_dir = os.path.join('models_results', best_model_name)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    X_test_final['pm2.5'].plot(figsize=(10, 5),\n",
    "                                    ms=1,\n",
    "                                    lw=1,\n",
    "                                    colormap='viridis',\n",
    "                                    legend=True,\n",
    "                                    title='Predictions for model '+str(best_model_name) + \" 100 steps ahead\")\n",
    "\n",
    "    future_w_features['pred'].plot(figsize=(10, 5),\n",
    "                                    ms=1,\n",
    "                                    lw=1,\n",
    "                                    legend=True,\n",
    "                                    title='Predictions for model '+str(best_model_name) + \" 100 steps ahead\")\n",
    "    \n",
    "    plt.savefig(os.path.join(model_dir, 'predictions.png'))\n",
    "    plt.show()\n",
    "    \n",
    "def evaluate_model(model,y_test,y_pred):\n",
    "    metrics = {}\n",
    "    \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    metrics['MSE'] = mse\n",
    "    metrics['RMSE'] = rmse\n",
    "    metrics['MAE'] = mae\n",
    "    metrics['R2'] = r2\n",
    "    return metrics\n",
    "\n",
    "def create_folder_structure():\n",
    "    folders = [\n",
    "        #'data',\n",
    "        #'data',\n",
    "        #'notebooks',\n",
    "        #'src',\n",
    "        'models_results',\n",
    "        'models_pkl'\n",
    "    ]\n",
    "    for folder in folders:\n",
    "        os.makedirs(folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c50cd30",
   "metadata": {},
   "source": [
    "## 7. Model Training and Evaluation - \n",
    "\n",
    "1. We reload and preprocess the data to ensure a fresh start.\n",
    "2. The data is split into training and testing sets, with the test set containing data from December 28, 2014 onwards.\n",
    "3. We create time-based features and lag features.\n",
    "4. An grid of models  is defined with specific hyperparameters.\n",
    "5. We use TimeSeriesSplit with 3 folds for cross-validation.\n",
    "6. The model is trained and evaluated using our custom functions.\n",
    "7. We print the performance metrics and generate predictions for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4585ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "# for docker\n",
    "df = pd.read_csv('beijing_pm_25.csv')\n",
    "display(df.head())\n",
    "\n",
    "# for local\n",
    "#df = pd.read_csv('/Users/andresjr/Documents/capgemini/Task_2_new/data/beijing_pm_25.csv')\n",
    "#display(df.head())\n",
    "\n",
    "\n",
    "X = preprocess_data(df)\n",
    "\n",
    "# The ending date for the test set is chosen as: 2014-12-28. The data before this date is used for training\n",
    "X_ = X[X.index < '2014-12-28']\n",
    "\n",
    "# The starting date for the test set is chosen as: 2014-12-28. The data after this date is used for testing\n",
    "X_test_final = X[X.index >= '2014-12-28']\n",
    "\n",
    "X = X_.copy()\n",
    "\n",
    "\n",
    "# the test size is chosen as half year days and the gap between the training and test is chosen to 0. \n",
    "tss = TimeSeriesSplit(n_splits=3, test_size=12*365*1, gap=12)\n",
    "\n",
    "models = {\n",
    "        'XGBoost': xgb.XGBRegressor(base_score=0.5, booster='gbtree', \n",
    "                                    n_estimators=1500, \n",
    "                                    objective='reg:linear',\n",
    "                                    max_depth=5,\n",
    "                                    learning_rate=0.01),\n",
    "        \"randomforest\": RandomForestRegressor(n_estimators=100, max_depth=3),\n",
    "        \"adaboost\": AdaBoostRegressor(n_estimators=100),\n",
    "    }\n",
    "\n",
    "FEATURES = ['DEWP','TEMP', 'PRES', 'Iws','dayofyear', 'hour', 'dayofweek', 'quarter', 'month','year'] \n",
    "\n",
    "\n",
    "model_store, metrics = training(models, X, FEATURES, tss)\n",
    "\n",
    "metrics_df = metrics_to_df(metrics)\n",
    "\n",
    "\n",
    "for name, model in model_store.items():\n",
    "    future_predictions(name,model,X,X_test_final)\n",
    "    model_dir_pkl = os.path.join('models_pkl', name)\n",
    "    os.makedirs(model_dir_pkl, exist_ok=True)\n",
    "    joblib.dump(model, os.path.join(model_dir_pkl, 'model.pkl'))\n",
    "\n",
    "metrics_df.to_csv('models_results/metrics.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a6d127",
   "metadata": {},
   "source": [
    "## Best model: XGBoost\n",
    "\n",
    "The best model is XGBoost. This is likely because has ability to capture complex patterns in the time series data, effectively utilize the engineered features (including lag features), and regularization techniques that help prevent overfitting. \n",
    "\n",
    "As can be seen from the generated plot with the predictions, it manages to get an overview of the trend. I believe that with some of the potential improvements described in the next section (like hyperparameter tuning) or more complex models/features, a better estimate can be obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306a423e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_name = metrics_df.iloc[0]['model']\n",
    "best_model = model_store[best_model_name]\n",
    "\n",
    "print(\"This is the best model:\", model_store)\n",
    "\n",
    "future_predictions(best_model_name,best_model,X,X_test_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5913ff7c",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "This notebook demonstrates a comprehensive approach to analyzing and predicting PM2.5 levels in Beijing. We've covered several key aspects of time series analysis and machine learning:\n",
    "\n",
    "1. Data preprocessing and feature engineering\n",
    "2. Checking for seasonality and stationarity in the time series\n",
    "3. Implementing time series cross-validation\n",
    "4. Training and evaluating multiple models, including XGBoost, Random Forest, and AdaBoost.\n",
    "5. Visualizing model performance and predictions\n",
    "\n",
    "The results provide insights into the effectiveness of different models in predicting PM2.5 levels, which can be valuable for air quality management and public health initiatives in Beijing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692050a3",
   "metadata": {},
   "source": [
    "## 9. Potential Improvements\n",
    "\n",
    "To further enhance this analysis, consider the following improvements:\n",
    "\n",
    "1. Hyperparameter tuning: Use techniques like GridSearchCV to optimize model parameters.\n",
    "2. Feature importance analysis: Identify which features contribute most to the predictions.\n",
    "3. Ensemble methods: Combine predictions from multiple models to potentially improve accuracy.\n",
    "4. Deep learning approaches: Experiment with LSTM or other neural network architectures for time series prediction.\n",
    "5. Use Prophet to get model prediction, a powerful library by Meta that yields much better predictions.\n",
    "5. External data integration: Incorporate additional relevant data sources, such as traffic patterns or industrial activity.\n",
    "6. Longer-term forecasting: Extend the prediction horizon to provide longer-term air quality forecasts.\n",
    "7. Spatial analysis: If data is available, include spatial variations in PM2.5 levels across different areas of Beijing.\n",
    "\n",
    "These enhancements could provide even more comprehensive and accurate predictions of PM2.5 levels, further supporting air quality management efforts in Beijing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 211.286735,
   "end_time": "2022-08-01T21:32:51.813564",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-08-01T21:29:20.526829",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
